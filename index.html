<!DOCTYPE html>
<html lang="en-US">

<head>
  <title> Kazuki Osawa </title>
  <meta charset="utf-8">
  <meta name="description" content="Kazuki Osawa's homepage">
  <link href="style.css" rel="stylesheet" type="text/css">
</head>

<body>
  <div>
    <table align="center">
      <tbody>
        <tr>
            <td width="80%">
              <div>
                <h1>Kazuki Osawa / 大沢 和樹</h1>
              </div>
              <p>
                Tokyo Institute of Technology, Japan <br>
                Institute for Infocomm Research (I²R), A*STAR, Singapore <br>
                東京工業大学 <br>
                シンガポール科学技術研究庁 <br>
                <br>
                Email: oosawa.k.ad at m dot titech dot ac dot jp
                <br>
                <a href="https://scholar.google.com/citations?user=IHdZHh8AAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td width="20%">
              <img src="./kazukiosawa.png" width="100%">
            </td>
        </tr>
      </tbody>
    </table>

    <hr>
    <h2>About Me</h2>   
    <p>I am a Ph.D. candidate in Department of Computer Science, School of Computing, Tokyo Institute of Technology, supervised by <a href="http://www.rio.gsic.titech.ac.jp/en/member/yokota.html">Rio Yokota</a>. 
    I am now working as a Research Assistant at the <a href="https://www.a-star.edu.sg/i2r">Institute for Infocomm Research (I²R )</a>, A*STAR, advised by <a href="http://ai.stanford.edu/~csfoo/">Chuan-Sheng Foo</a> and <a href="http://vijaychan.github.io/">Vijay Chandrasekhar</a>.</p>

    <p>I earned B.S. (2016) and M.S. (2018) in Computer Science from Tokyo Institute of Technology.</p>

    <h2>Research Interests</h2>
    <p>My research interests include optimization, approximation theory, high-performance computing (HPC), and deep learning.
    My goal in the Ph.D. course is to propose theoretical optimization methods for deep neural networks which are compatible with research in the HPC field and to become a bridge between these fields.</p>
    <p>Topics:</p>
    <ul>
      <li>Second-order optimization methods for deep learning</li>
      <li>Large-scale distributed computing</li>
      <li>Bayesian deep learning</li>
      <li>Low-rank approximation</li>
    </ul>

    <h2>Current Work</h2>
    <ul>
      <li>
          <a href="https://arxiv.org/abs/1811.12019">Second-order Optimization Method for Large Mini-batch: Training ResNet-50 on ImageNet in 35 Epochs</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          arXiv:1811.12019 [cs.LG], Nov. 2018.
      </li>
    </ul>

    <h2>Publications</h2>
    <ul>
      <li>
          <a href="https://link.springer.com/chapter/10.1007/978-3-319-68612-7_52">Evaluating the Compression Efficiency of the Filters in Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b> and Rio Yokota,
          <br>
          Artificial Neural Networks and Machine Learning – ICANN 2017, pp 459-466, Springer 2017.
      </li>
      <li>
          <a href="https://ieeexplore.ieee.org/document/8035076">Accelerating Matrix Multiplication in Deep Learning by Using Low-Rank Approximation</a>,
          <br>
          <b>Kazuki Osawa</b>, Akira Sekiya, Hiroki Naganuma, and Rio Yokota,
          <br>
          2017 International Conference on High Performance Computing & Simulation (HPCS), pp 186-192, IEEE 2017.
      </li>
    </ul>

    <h2>Blog Posts</h2>
    <ul>
        <li>
            <a href="https://medium.com/@osawa1021/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414">Introducing K-FAC and its application to Large-scale Deep Learning</a>,
            <br>
            Dec. 2018 (Medium)
        </li>
    </ul>

    <h2>CV</h2>
    My CV can be found <a href="./kazukiosawa_cv.pdf">here</a>. (Last updated: Dec 2018)

  </div>
</body>

</html>
