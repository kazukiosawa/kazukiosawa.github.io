<!DOCTYPE html>
<html lang="en-US">

<head>
  <title> Kazuki Osawa </title>
  <meta charset="utf-8">
  <meta name="description" content="Kazuki Osawa's homepage">
  <link href="style.css" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="tokyotechlogo.ico">
</head>

<body>
  <div>
    <table align="center">
      <tbody>
        <tr>
            <td width="55%">
              <div>
                <h1>Kazuki Osawa / 大沢 和樹</h1>
              </div>
              <p>
                <a href="https://scholar.google.com/citations?user=IHdZHh8AAAAJ&hl=en">Google Scholar</a>
                <br>
                <a href="https://github.com/kazukiosawa">GitHub</a>
                <br>
                <a href="./kazukiosawa_cv.pdf">Curriculum Vitae</a> 
                <br>
                <br>
                <br>
                <br>
                (last updated: 2023.04.28)
              </p>
            </td>
            <td width="45%">
                <figure>
                    <img src="./kazukiosawa_taki.jpg" width="100%">
                    <figcaption class="prof" align="center">Photo by <a href="https://www.y1r.org/">Yuichiro Ueno</a></figcaption>
                </figure>
            </td>
        </tr>
      </tbody>
    </table>

    <hr>
    <p>
    I am a Research Engineer at Google DeepMind.
    Previously, I was an <a href="https://ethz.ch/en/research/research-promotion/eth-fellowships.html">ETH Postdoctoral Fellow<a/> at ETH Zurich, working with <a href="https://htor.inf.ethz.ch/">Prof. Torsten Hoefler</a> in the <a href="https://spcl.inf.ethz.ch/">Scalable Parallel Computing Laboratory</a>.
    I received a Ph.D. in computer science at the Tokyo Institute of Technology where I was fortunate to be advised by <a href="https://www.rio.gsic.titech.ac.jp/en/member/yokota.html">Prof. Rio Yokota</a>.<br>

    <h2>Publications</h2>
    <ul>
      <li>
          <a href="https://arxiv.org/abs/2210.02720">Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias</a>,
          <br>
          Ryo Karakida, Tomoumi Takase, Tomohiro Hayase, and <b>Kazuki Osawa</b>,
          <br>
          International Conference on Machine Learning (<b>ICML 2023</b>).
      </li>
      <li>
          <a href="https://arxiv.org/abs/2211.14133">PipeFisher: Efficient Training of Large Language Models Using Pipelining and Fisher Information Matrices</a>,
          <br>
          <b>Kazuki Osawa</b>, Shigang Li, and Torsten Hoefler,
          <br>
          Sixth Conference on Machine Learning and Systems (<b>MLSys 2023</b>).
      </li>
      <li>
          <a href="https://openreview.net/forum?id=p0sMj8oH2O">Neural Graph Databases</a>,
          <br>
          Maciej Besta, Patrick Iff, Florian Scheidl, <b>Kazuki Osawa</b>, Nikoli Dryden, Michal Podstawski, Tiancheng Chen, and Torsten Hoefler,
          <br>
          Learning on Graphs Conference (<b>LoG 2022</b>).
      </li>
      <li>
          <a href="https://arxiv.org/abs/2209.06979">Efficient Quantized Sparse Matrix Operations on Tensor Cores</a>,
          <br>
          Shigang Li, <b>Kazuki Osawa</b>, and Torsten Hoefler,
          <br>
          International Conference for High Performance Computing, Networking, Storage and Analysis (<b>SC22</b>), best paper finalist.
      </li>
      <li>
          <a href="https://ieeexplore.ieee.org/document/9123671">Scalable and Practical Natural Gradient for Large-Scale Deep Learning</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Chuan-Sheng Foo, and Rio Yokota,
          <br>
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>), vol. 44, no. 1, pp. 404-415, 1 Jan. 2022.
      </li>
      <li>
          <a href="https://arxiv.org/abs/2010.00879">Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks</a>,
          <br>
          Ryo Karakida and <b>Kazuki Osawa</b>,
          <br>
          Advances in Neural Information Processing Systems (<b>NeurIPS 2020</b>), oral presentation.  
          [<a href="https://crossminds.ai/video/understanding-approximate-fisher-information-for-fast-convergence-of-natural-gradient-descent-in-wide-neural-networks-606ff0aff43a7f2f827c184b/">video</a>][<a href="https://github.com/kazukiosawa/ngd_in_wide_nn">code</a>]
      </li>
      <li>
          <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403265">Rich Information is Affordable: A Systematic Performance Analysis of Second-order Optimization Using K-FAC</a>,
          <br>
          Yuichiro Ueno, <b>Kazuki Osawa</b>, Yohei Tsuji, Akira Naruse, and Rio Yokota,
          <br>
          ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD 2020</b>).
      </li>
      <li>
          <a href="https://arxiv.org/abs/1906.02506">Practical Deep Learning with Bayesian Principles</a>,
          <br>
          <b>Kazuki Osawa</b>, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota, and Mohammad Emtiyaz Khan,
          <br>
          Advances in Neural Information Processing Systems (<b>NeurIPS 2019</b>) [<a href="https://github.com/team-approx-bayes/dl-with-bayes/blob/master/neurips2019_poster.pdf">poster</a>][<a href="https://github.com/team-approx-bayes/dl-with-bayes">code</a>] 
      </li>
      <li>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Osawa_Large-Scale_Distributed_Second-Order_Optimization_Using_Kronecker-Factored_Approximate_Curvature_for_Deep_CVPR_2019_paper.html">Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2019</b>) [<a href="./cvpr19_poster.pdf">poster</a>][<a href="https://github.com/tyohei/chainerkfac">code</a>]
      </li>
      <li>
          <a href="https://dl.acm.org/doi/abs/10.1145/3339186.3339202">Performance Optimizations and Analysis of Distributed Deep Learning with Approximated Second-Order Optimization Method</a>,
          <br>
          Yohei Tsuji, <b>Kazuki Osawa</b>, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          The 48th International Conference on Parallel Processing: Workshops (<b>ICPP 2019</b> Workshop) 
      </li>
      <li>
          <a href="https://link.springer.com/chapter/10.1007/978-3-319-68612-7_52">Evaluating the Compression Efficiency of the Filters in Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b> and Rio Yokota,
          <br>
          Artificial Neural Networks and Machine Learning – <b>ICANN 2017</b> , pp 459-466, Springer 2017.
      </li>
    </ul>

    <h2>Talks</h2>
    <ul>
        <li>
            Invited talk at the <a href="https://www.isc-hpc.com/">ISC High Performance 2021 Digital</a>, July 1st, 2021
            <br>
            Second-order Optimizaiton for Large-scale Deep Learning (Distributed K-FAC for training ResNet-50 on ImageNet) [<a href="https://www.dropbox.com/s/93pa31obmks4fvi/isc21_kazuki_osawa.mp4?dl=0">video</a>][<a href="https://www.dropbox.com/s/xup17nqbd89y4n4/isc21_kazuki_osawa.pdf?dl=0">sildes</a>]
        </li>
    </ul>

    <h2>Service</h2>
    <ul>
        <li>
            Served as a reviewer at Neural Networks (2021), NeurIPS 2021, ICLR 2022, NeurIPS 2022, ICLR 2023, NeurIPS 2023, and ICML 2023. 
        </li>
        <li>
            Selected as a Highlighted Reviewer at <a href="https://iclr.cc/Conferences/2022/Reviewers">ICLR 2022 </a> (top ~8%), Apr, 2022
        </li>
    </ul>

    <h2>Open Source</h2>
    <ul>
        <li>
            <a href="https://github.com/kazukiosawa/asdfghjkl">Automatic Second-order Differentiation Library (ASDL)</a> [<a href="https://www.dropbox.com/s/i18vltdnfbshk7o/asdl.pdf?dl=0">slides</a>][<a href="https://arxiv.org/abs/2305.04684">paper</a>]
            <br>
            A PyTorch extension for computing various metrics (Hessian, Jacobian, Fisher information matrix, gradient covariance, NTK, etc) and performing second-order optimization in deep learning.
        </li>
        <li>
            <a href="https://github.com/tyohei/chainerkfac">chainerkfac</a>
            <br>
            A Chainer extension for distributed K-FAC
        </li>
        <li>
            <a href="https://github.com/cybertronai/pytorch-sso">PyTorch-SSO: Scalable Second-Order methods in PyTorch</a>
            <br>
            A PyTorch extension for second-order optimization, Bayesian inference and distributed training
        </li>
        <li>
            <a href="https://github.com/cybertronai/autograd-lib">autograd-lib</a>
            <br>
            A library to simplify gradient computations in PyTorch
        </li>
        <li>
            <a href="https://github.com/team-approx-bayes/dl-with-bayes">dl-with-bayes</a>
            <br>
            A code collection for Deep Learning with Bayesian Principles (variational inference) in PyTorch
        </li>
        <li>
            <a href="https://github.com/kazukiosawa/ngd_in_wide_nn">ngd_in_wide_nn</a>
            <br>
           JAX-/NumPy-based implementations of Natural Gradient Descent with exact/approximate Fisher information matrix in parameter-/function-space of finite-/infinite-width neural networks. 
        </li>
    </ul>

  </div>
</body>

</html>
