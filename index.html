<!DOCTYPE html>
<html lang="en-US">

<head>
  <title> Kazuki Osawa </title>
  <meta charset="utf-8">
  <meta name="description" content="Kazuki Osawa's homepage">
  <link href="style.css" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="tokyotechlogo.ico">
</head>

<body>
  <div>
    <table align="center">
      <tbody>
        <tr>
            <td width="55%">
              <div>
                <h1>Kazuki Osawa / 大沢 和樹</h1>
              </div>
              <p>
                Department of Computer Science, Tokyo Institute of Technology<br>
                Research Fellow of Japan Society for the Promotion of Science<br>
                東京工業大学 情報理工学院 情報工学系<br>
                日本学術振興会 特別研究員 DC2 <br>
                <br>
                Email: oosawa.k.ad at m.titech.ac.jp
                <br>
                <a href="https://scholar.google.com/citations?user=IHdZHh8AAAAJ&hl=en">Google Scholar</a>
                <br>
                <a href="./kazukiosawa_cv.pdf">Curriculum Vitae</a> 
              </p>
            </td>
            <td width="45%">
                <figure>
                    <img src="./kazukiosawa_taki.jpg" width="100%">
                    <figcaption class="prof" align="center">Photo by Yuichiro Ueno @ 浄蓮の滝</figcaption>
                </figure>
            </td>
        </tr>
      </tbody>
    </table>

    <hr>
    <p>
    I am a Ph.D. candidate in the Department of Computer Science at the Tokyo Institute of Technology, supervised by <a href="http://www.rio.gsic.titech.ac.jp/en/member/yokota.html">Rio Yokota</a>.
    I am a Research Fellow of <a href="https://www.jsps.go.jp/english/e-pd/index.html">Japan Society for the Promotion of Science (JSPS)</a> (DC2).
    I am currently advised by <a href="http://ai.stanford.edu/~csfoo/">Chuan-Sheng Foo</a> and <a href="http://vijaychan.github.io/">Vijay Chandrasekhar</a> at the <a href="https://www.a-star.edu.sg/i2r">Institute for Infocomm Research (I²R)</a>, A*STAR, Singapore. 
    </p>

    <ul>
        <li>4/2019 - present: Research Fellow of JSPS (DC2)</li>
        <li>10/2018 - 3/2019: Research Assistant at the I²R, A*STAR (awarded <a href="https://www.a-star.edu.sg/Scholarships/For-Graduate-Studies/A-STAR-Research-Attachment-Programme-ARAP">A*STAR Research Attachment Programme</a>)</li>
        <li>4/2018 - present: Ph.D. candidate in the Department of Computer Science at the Tokyo Institute of Technology</li>
        <li>4/2016 - 3/2018: M.Eng. in the Department of Computer Science at the Tokyo Institute of Technology</li>
        <li>4/2012 - 3/2016: B.Eng. in the Department of Computer Science at the Tokyo Institute of Technology</li>
    </ul>

    <h2>Research Interests</h2>
    <p>My research interests include optimization, approximation theory, high-performance computing (HPC), and deep learning.
    My goal in the Ph.D. course is to propose theoretical optimization methods for deep neural networks which are compatible with research in the HPC field and to become a bridge between these fields.</p>
    <p>Topics:</p>
    <ul>
      <li>Second-order optimization methods for deep learning</li>
      <li>Large-scale distributed computing</li>
      <li>Approximate Bayesian inference</li>
    </ul>

    <h2>Preprints</h2>
    <ul>
      <li>
          <a href="https://arxiv.org/abs/1906.02506">Practical Deep Learning with Bayesian Principles</a>,
          <br>
          <b>Kazuki Osawa</b>, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota, and Mohammad Emtiyaz Khan,
          <br>
          arXiv preprint arXiv:1906.02506, 2019. 
      </li>
    </ul>

    <h2>Publications (Refereed)</h2>
    <ul>
      <li>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Osawa_Large-Scale_Distributed_Second-Order_Optimization_Using_Kronecker-Factored_Approximate_Curvature_for_Deep_CVPR_2019_paper.pdf">Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          IEEE/CVF <b>CVPR</b> 2019, Long Beach, CA (AR:25.2% 1300/5160) [<a href="./cvpr19_poster.pdf">poster</a>][<a href="https://github.com/tyohei/chainerkfac">code</a>]
      </li>
      <li>
          <a href="https://link.springer.com/chapter/10.1007/978-3-319-68612-7_52">Evaluating the Compression Efficiency of the Filters in Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b> and Rio Yokota,
          <br>
          Artificial Neural Networks and Machine Learning – <b>ICANN</b> 2017, pp 459-466, Springer 2017.
      </li>
      <li>
          <a href="https://ieeexplore.ieee.org/document/8035076">Accelerating Matrix Multiplication in Deep Learning by Using Low-Rank Approximation</a>,
          <br>
          <b>Kazuki Osawa</b>, Akira Sekiya, Hiroki Naganuma, and Rio Yokota,
          <br>
          2017 International Conference on High Performance Computing & Simulation (<b>HPCS</b>), pp 186-192, IEEE 2017.
      </li>
    </ul>

    <h2>Codes</h2>
    <ul>
        <li>
            chainerkfac
            [<a href="https://github.com/tyohei/chainerkfac">GitHub</a>]
            <br>
            Chainer extension for distributed K-FAC
        </li>
    </ul>

    <h2>Blog Posts</h2>
    <ul>
        <li>
            <a href="https://medium.com/@osawa1021/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414">Introducing K-FAC — A Second-Order Optimization Method for Large-Scale Deep Learning</a>,
            <br>
            Dec. 2018 (Towards Data Science, Medium)
        </li>
        <li>
            <a href="https://medium.com/@osawa1021/k-fac%E3%81%A8%E3%81%AF-de30537f7096">K-FACとは？〜大規模深層学習のための二次最適化の実現〜</a>,
            <br>
            Dec. 2018 (Medium)
        </li>
    </ul>

  </div>
</body>

</html>
