<!DOCTYPE html>
<html lang="en-US">

<head>
  <title> Kazuki Osawa </title>
  <meta charset="utf-8">
  <meta name="description" content="Kazuki Osawa's homepage">
  <link href="style.css" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="tokyotechlogo.ico">
</head>

<body>
  <div>
    <table align="center">
      <tbody>
        <tr>
            <td width="55%">
              <div>
                <h1>Kazuki Osawa / 大沢 和樹</h1>
              </div>
              <p>
                Department of Computer Science, ETH Zurich<br>
                Scalable Parallel Computing Laboratory<br>
                ETH Zurich Postdoctoral Fellow<br>
                <br>
                Email: kazuki.osawa at inf.ethz.ch
                <br>
                <a href="https://scholar.google.com/citations?user=IHdZHh8AAAAJ&hl=en">Google Scholar</a>
                <br>
                <a href="./kazukiosawa_cv.pdf">Curriculum Vitae</a> 
              </p>
            </td>
            <td width="45%">
                <figure>
                    <img src="./kazukiosawa_taki.jpg" width="100%">
                    <figcaption class="prof" align="center">Photo by <a href="https://www.y1r.org/">Yuichiro Ueno</a></figcaption>
                </figure>
            </td>
        </tr>
      </tbody>
    </table>

    <hr>
    <p>
    I am an ETH Zurich Postdoctoral Fellow at ETH Zurich, working with <a href="https://htor.inf.ethz.ch/">Torsten Hoefler</a> in the <a href="https://spcl.inf.ethz.ch/">Scalable Parallel Computing Laboratory</a>.
    I received a Ph.D. in computer science at the Tokyo Institute of Technology where I was fortunate to be advised by <a href="https://www.rio.gsic.titech.ac.jp/en/member/yokota.html">Rio Yokota</a>.<br>

    <h2>Research Interests</h2>
    <ul>
      <li>Scaling matrix-based algorithms in deep learning</li>
      <ul>
        <li>Second-order optimization, natural gradient method</li>
        <li>Approximate Bayesian inference</li>
        <li>Generalization measures</li>
        <li>Hyper-parameter optimization</li>
        <li>etc.</li>
      </ul>
      <li>Large-scale distributed parallel computing</li>
      <li>Performance optimization</li>
    </ul>

    <h2>Education</h2>
    <dl>
        <dt>2018 - 2021</dt>
            <dd>D.Eng. in Computer Science at the Tokyo Institute of Technology, Japan</dd>
        <dt>2016 - 2018</dt>
            <dd>M.Eng. in Computer Science at the Tokyo Institute of Technology, Japan</dd>
        <dt>2012 - 2016</dt>
            <dd>B.Eng. in Computer Science at the Tokyo Institute of Technology, Japan</dd>
    </dl>

    <h2>Experience</h2>
    <dl>
        <dt>Apr 2021 -</dt>
            <dd>ETH Zurich Postdoctoral Fellow, Switzerland</dd>
        <dt>Apr 2019 - Mar 2021</dt>
            <dd>Research Fellow of JSPS DC2, Japan</dd>
        <dt>Jan 2020 - Mar 2020</dt>
            <dd>Student trainee at the <a href="https://www.airc.aist.go.jp/en/mlrt/">Machine Learning Research Team</a>, AIRC, AIST, Japan (with <a href="https://sites.google.com/view/ryokarakida/english">Ryo Karakida</a>)</dd>
        <dt>Nov 2019 - Feb 2020</dt>
            <dd>Student trainee at the <a href="https://www.riken.jp/en/research/labs/aip/generic_tech/approx_bayes_infer/index.html">Approximate Bayesian Inference Team</a>, RIKEN AIP, Japan (with <a href="https://emtiyaz.github.io/">Emtiyaz Khan</a>)</dd>
        <dt>Nov - Dec 2019</dt>
            <dd>Research Intern at the <a href="https://www.d-itlab.co.jp/">DENSO IT Laboratory</a>, Japan (with <a href="https://dblp.org/pers/hd/s/Sato:Ikuro">Ikuro Sato</a>)</dd>
        <dt>2018 - 2019</dt>
            <dd>Research Assistant at the <a href="https://www.a-star.edu.sg/i2r/">I²R</a>, A*STAR, Singapore (with <a href="http://ai.stanford.edu/~csfoo/">Chuan-Sheng Foo</a>)</dd>
    </dl>

    <h2>Publications</h2>
    <ul>
      <li>
          (<font color='red'>new</font>) <a href="https://arxiv.org/abs/2010.00879">Understanding Approximate Fisher Information for Fast Convergence of Natural Gradient Descent in Wide Neural Networks</a>,
          <br>
          Ryo Karakida and <b>Kazuki Osawa</b>,
          <br>
          Advances in Neural Information Processing Systems (<b>NeurIPS 2020</b>), oral presentation.  
          [<a href="https://github.com/kazukiosawa/ngd_in_wide_nn">code</a>]
      </li>
      <li>
          <a href="https://ieeexplore.ieee.org/document/9123671">Scalable and Practical Natural Gradient for Large-Scale Deep Learning</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Chuan-Sheng Foo, and Rio Yokota,
          <br>
          IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>).
      </li>
      <li>
          <a href="https://dl.acm.org/doi/abs/10.1145/3394486.3403265">Rich Information is Affordable: A Systematic Performance Analysis of Second-order Optimization Using K-FAC</a>,
          <br>
          Yuichiro Ueno, <b>Kazuki Osawa</b>, Yohei Tsuji, Akira Naruse, and Rio Yokota,
          <br>
          ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<b>KDD 2020</b>).
      </li>
      <li>
          <a href="https://arxiv.org/abs/1906.02506">Practical Deep Learning with Bayesian Principles</a>,
          <br>
          <b>Kazuki Osawa</b>, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota, and Mohammad Emtiyaz Khan,
          <br>
          Advances in Neural Information Processing Systems (<b>NeurIPS 2019</b>) [<a href="https://github.com/team-approx-bayes/dl-with-bayes/blob/master/neurips2019_poster.pdf">poster</a>][<a href="https://github.com/team-approx-bayes/dl-with-bayes">code</a>] 
      </li>
      <li>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Osawa_Large-Scale_Distributed_Second-Order_Optimization_Using_Kronecker-Factored_Approximate_Curvature_for_Deep_CVPR_2019_paper.html">Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2019</b>) [<a href="./cvpr19_poster.pdf">poster</a>][<a href="https://github.com/tyohei/chainerkfac">code</a>]
      </li>
      <li>
          <a href="https://dl.acm.org/doi/abs/10.1145/3339186.3339202">Performance Optimizations and Analysis of Distributed Deep Learning with Approximated Second-Order Optimization Method</a>,
          <br>
          Yohei Tsuji, <b>Kazuki Osawa</b>, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          The 48th International Conference on Parallel Processing: Workshops (<b>ICPP 2019</b> Workshop) 
      </li>
      <li>
          <a href="https://link.springer.com/chapter/10.1007/978-3-319-68612-7_52">Evaluating the Compression Efficiency of the Filters in Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b> and Rio Yokota,
          <br>
          Artificial Neural Networks and Machine Learning – <b>ICANN 2017</b> , pp 459-466, Springer 2017.
      </li>
    </ul>

    <h2>Open Source</h2>
    <ul>
        <li>
            <a href="https://github.com/tyohei/chainerkfac">chainerkfac</a>
            <br>
            A Chainer extension for distributed K-FAC
        </li>
        <li>
            <a href="https://github.com/cybertronai/pytorch-sso">PyTorch-SSO: Scalable Second-Order methods in PyTorch</a>
            <br>
            A PyTorch extension for second-order optimization, Bayesian inference and distributed training
        </li>
        <li>
            <a href="https://github.com/cybertronai/autograd-lib">autograd-lib</a>
            <br>
            A library to simplify gradient computations in PyTorch
        </li>
        <li>
            <a href="https://github.com/team-approx-bayes/dl-with-bayes">dl-with-bayes</a>
            <br>
            A code collection for Deep Learning with Bayesian Principles (variational inference) in PyTorch
        </li>
        <li>
            <a href="https://github.com/kazukiosawa/ngd_in_wide_nn">ngd_in_wide_nn</a>
            <br>
           JAX-/NumPy-based implementations of Natural Gradient Descent with exact/approximate Fisher information matrix in parameter-/function-space of finite-/infinite-width neural networks. 
        </li>
    </ul>

    <h2>Blog Posts</h2>
    <ul>
        <li>
            <a href="https://medium.com/@osawa1021/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414">Introducing K-FAC — A Second-Order Optimization Method for Large-Scale Deep Learning</a>,
            <br>
            Dec. 2018 (Towards Data Science, Medium)
        </li>
        <li>
            <a href="https://medium.com/@osawa1021/k-fac%E3%81%A8%E3%81%AF-de30537f7096">K-FACとは？〜大規模深層学習のための二次最適化の実現〜</a>,
            <br>
            Dec. 2018 (Medium)
        </li>
    </ul>

  </div>
</body>

</html>
