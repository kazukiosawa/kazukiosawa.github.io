<!DOCTYPE html>
<html lang="en-US">

<head>
  <title> Kazuki Osawa </title>
  <meta charset="utf-8">
  <meta name="description" content="Kazuki Osawa's homepage">
  <link href="style.css" rel="stylesheet" type="text/css">
  <link rel="shortcut icon" href="tokyotechlogo.ico">
</head>

<body>
  <div>
    <table align="center">
      <tbody>
        <tr>
            <td width="55%">
              <div>
                <h1>Kazuki Osawa / 大沢 和樹</h1>
              </div>
              <p>
                Department of Computer Science, Tokyo Institute of Technology<br>
                Research Fellow of JSPS DC2<br>
                <br>
                Email: oosawa.k.ad at m.titech.ac.jp
                <br>
                <a href="https://scholar.google.com/citations?user=IHdZHh8AAAAJ&hl=en">Google Scholar</a>
                <br>
                <a href="./kazukiosawa_cv.pdf">Curriculum Vitae</a> 
              </p>
            </td>
            <td width="45%">
                <figure>
                    <img src="./kazukiosawa_taki.jpg" width="100%">
                    <figcaption class="prof" align="center">Photo by <a href="https://www.y1r.org/">Yuichiro Ueno</a></figcaption>
                </figure>
            </td>
        </tr>
      </tbody>
    </table>

    <hr>
    <p>
    I am a Ph.D. student in Computer Science at the Tokyo Institute of Technology, supervised by <a href="https://sites.google.com/site/rioyokota/home">Rio Yokota</a>.<br>
    I am a Research Fellow of <a href="https://www.jsps.go.jp/english/e-pd/index.html">Japan Society for the Promotion of Science</a> (JSPS) DC2.<br>

    <h2>Research Interests</h2>
    <p>
    My goal in the doctoral course is to become a bridge between the principled machine learning (ML) field and the high-performance computing (HPC) field for a deeper understanding of the learning principles in practical settings.
    </p>
    <p>Topics:</p>
    <ul>
      <li>Second-order methods in deep learning</li>
      <li>Large-scale distributed computing</li>
      <li>Approximate Bayesian inference</li>
    </ul>

    I am supported by <a href="https://www.jsps.go.jp/english/e-grants/">JSPS KAKENHI</a> Grant Number JP19J13477 on <i>Natural gradient approximation methods for large-scale Bayesian deep learning</i>.<br>

    <h2>Education</h2>
    <dl>
        <dt>Apr 2018 - </dt>
            <dd>Ph.D. student in Computer Science at the Tokyo Institute of Technology, Japan</dd>
        <dt>2016 - 2018</dt>
            <dd>M.Eng. in Computer Science at the Tokyo Institute of Technology, Japan</dd>
        <dt>2012 - 2016</dt>
            <dd>B.Eng. in Computer Science at the Tokyo Institute of Technology, Japan</dd>
    </dl>

    <h2>Experience</h2>
    <dl>
        <dt>Apr 2019 - </dt>
            <dd>Research Fellow of JSPS DC2, Japan</dd>
        <dt>Nov 2019 - Feb 2020</dt>
            <dd>Student trainee at the <a href="https://www.riken.jp/en/research/labs/aip/generic_tech/approx_bayes_infer/index.html">Approximate Bayesian Inference Team</a>, RIKEN AIP, Japan (with <a href="https://emtiyaz.github.io/">Emtiyaz Khan</a>)</dd>
        <dt>Nov - Dec 2019</dt>
            <dd>Research Intern at the <a href="https://www.d-itlab.co.jp/">DENSO IT Laboratory</a>, Japan (with <a href="https://dblp.org/pers/hd/s/Sato:Ikuro">Ikuro Sato</a>)</dd>
        <dt>2018 - 2019</dt>
            <dd>Research Assistant at the <a href="https://www.a-star.edu.sg/i2r/">I²R</a>, A*STAR, Singapore (with <a href="http://ai.stanford.edu/~csfoo/">Chuan-Sheng Foo</a>)</dd>
    </dl>

    <h2>Preprints</h2>
    <ul>
        <li>
            <a href="./scalable_practical_ngd.pdf">Scalable and Practical Natural Gradient for Large-Scale Deep Learning</a>
        </li>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Chuan-Sheng Foo, and Rio Yokota 
    </ul>

    <h2>Publications</h2>
    <ul>
      <li>
          <a href="https://arxiv.org/abs/1906.02506">Practical Deep Learning with Bayesian Principles</a>,
          <br>
          <b>Kazuki Osawa</b>, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard E. Turner, Rio Yokota, and Mohammad Emtiyaz Khan,
          <br>
          Conference on Neural Information Processing Systems (<b>NeurIPS 2019</b>) [<a href="https://github.com/team-approx-bayes/dl-with-bayes/blob/master/neurips2019_poster.pdf">poster</a>][<a href="https://github.com/team-approx-bayes/dl-with-bayes">code</a>] 
      </li>
      <li>
          <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Osawa_Large-Scale_Distributed_Second-Order_Optimization_Using_Kronecker-Factored_Approximate_Curvature_for_Deep_CVPR_2019_paper.html">Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b>, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR 2019</b>) [<a href="./cvpr19_poster.pdf">poster</a>][<a href="https://github.com/tyohei/chainerkfac">code</a>]
      </li>
      <li>
          <a href="https://dl.acm.org/doi/abs/10.1145/3339186.3339202">Performance Optimizations and Analysis of Distributed Deep Learning with Approximated Second-Order Optimization Method</a>,
          <br>
          Yohei Tsuji, <b>Kazuki Osawa</b>, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka,
          <br>
          The 48th International Conference on Parallel Processing: Workshops (<b>ICPP 2019</b> Workshop) 
      </li>
      <li>
          <a href="https://link.springer.com/chapter/10.1007/978-3-319-68612-7_52">Evaluating the Compression Efficiency of the Filters in Convolutional Neural Networks</a>,
          <br>
          <b>Kazuki Osawa</b> and Rio Yokota,
          <br>
          Artificial Neural Networks and Machine Learning – <b>ICANN 2017</b> , pp 459-466, Springer 2017.
      </li>
      <li>
          <a href="https://ieeexplore.ieee.org/document/8035076">Accelerating Matrix Multiplication in Deep Learning by Using Low-Rank Approximation</a>,
          <br>
          <b>Kazuki Osawa</b>, Akira Sekiya, Hiroki Naganuma, and Rio Yokota,
          <br>
          2017 International Conference on High Performance Computing & Simulation (<b>HPCS 2017</b>), pp 186-192, IEEE 2017.
      </li>
    </ul>

    <h2>Open Source</h2>
    <ul>
        <li>
            <a href="https://github.com/tyohei/chainerkfac">chainerkfac</a>
            <br>
            A Chainer extension for distributed K-FAC
        </li>
        <li>
            <a href="https://github.com/cybertronai/pytorch-sso">PyTorch-SSO: Scalable Second-Order methods in PyTorch</a>
            <br>
            A PyTorch extension for second-order optimization, Bayesian inference and distributed training
        </li>
        <li>
            <a href="https://github.com/cybertronai/autograd-lib">autograd-lib</a>
            <br>
            A library to simplify gradient computations in PyTorch
        </li>
        <li>
            <a href="https://github.com/team-approx-bayes/dl-with-bayes">dl-with-bayes</a>
            <br>
            A code collection for Deep Learning with Bayesian Principles (variational inference) in PyTorch
        </li>
    </ul>

    <h2>Blog Posts</h2>
    <ul>
        <li>
            <a href="https://medium.com/@osawa1021/introducing-k-fac-and-its-application-for-large-scale-deep-learning-4e3f9b443414">Introducing K-FAC — A Second-Order Optimization Method for Large-Scale Deep Learning</a>,
            <br>
            Dec. 2018 (Towards Data Science, Medium)
        </li>
        <li>
            <a href="https://medium.com/@osawa1021/k-fac%E3%81%A8%E3%81%AF-de30537f7096">K-FACとは？〜大規模深層学習のための二次最適化の実現〜</a>,
            <br>
            Dec. 2018 (Medium)
        </li>
    </ul>

  </div>
</body>

</html>
